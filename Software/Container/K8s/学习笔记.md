# K8s 学习笔记

<!-- 以下为临时资料 -->

第二篇资料

1. K8S是什么

K8S全称kubernetes，是由Google在2014年开源的生产级别的容器编排系统，或者说是微服务和云原生平台。虽说14年才开源，但实际上K8S是Google内部的容器编排系统Borg的开源版本，在Google内部已经用了十多年了。下面是一个关于K8S的Logo来源的小插曲。

    Kubernetes由谷歌在2014年首次对外宣布 。它的开发和设计都深受谷歌的Borg系统的影响，它的许多顶级贡献者之前也是Borg系统的开发者。在谷歌内部，Kubernetes的原始代号曾经是Seven，即星际迷航中友好的Borg(博格人)角色。Kubernetes标识中舵轮有七个轮辐就是对该项目代号的致意。

不过也有一个说法是，Docker的Logo是一个驮着集装箱的鲸鱼，也就是运输船，K8S的Logo是一个船舵，旨在引领着Docker（或者说容器技术）走向远方。

2. 简单了解K8S

看了很多官方文章，是真官方。官方什么意思呢，就是有可能看完了约等于没有看，一样的啥都不知道。

所以我想写这样一篇文章，给那些看完文档仍然不太理解或者说完全没了解过K8S的老铁一点小帮助。那么让我们回到最初对K8S的定义，它是一个微服务框架。

说到微服务框架，我们就不得不提一下目前业界十分主流的微服务框架，与这些你十分熟悉的框架进行对比，你就会很清晰的知道K8S能做什么了。目前很主流的微服务框架和平台有Spring Cloud、Dubbo和K8S。

Spring Cloud来自Netflix，Dubbo来自阿里，而K8S则来自Google。说的直观一点，这三个框架都是针对微服务的解决方案。可能有人会说，K8S不是一个容器编排系统吗？怎么跟Spring Cloud这种软件层面上的微服务框架做起了对比呢？

老铁别慌，等我们慢慢深入这个概念。

我们都知道，如果我们需要使用微服务，那么肯定少不了一些底层的基础设施的支撑，例如服务注册与发现、负载均衡、日志监控、配置管理、集群自愈和容错、弹性伸缩…等等。我没有列举完，如其实这些组件都可以统称为微服务的公共关注点。那我们是不是可以说，只要能够提供的这些功能，它就算一个微服务框架呢？

以上的大多数功能，K8S都是内置的。故我们可以说K8S是一个与Docker Swarm相类似的容器编排系统，但是由于K8S内置了微服务的解决方案，它同时也是一个功能完备的微服务框架。
2.1 Pod的概念

在Docker Swarm中，调度的最小单位是容器，而在K8S中，调度的最小是Pod，那啥是Pod呢？

Pod是K8S设计的一个全新的概念，在英文中的原意是表达一群鲸鱼或者是一个豌豆荚的意思。换句话说，一个Pod中可以运行一个或者多个容器。

在一个集群中，K8S会为每个Pod都分配一个集群内唯一的IP地址。因为K8S要求底层网络支持集群内的任意节点之间的两个Pod能够直接通信。这些容器共享当前Pod的文件系统和网络。而这些容器之所以能够共享，是因为Pod中有一个叫Pause的根容器，其余的用户业务容器都是共享这个根容器的IP和Volume。所以这些容器之间都可以通过localhost进行通信。

有人可能会问，为什么要引入根容器这个概念？那是因为如果没有根容器的话，当一个Pod中引入了多个容器的时候，我们应该用哪一个容器的状态来判断Pod的状态呢？所以才要引入与业务无关且不容易挂掉的Pause容器作为根容器，用根容器的状态来代表整个容器的状态。

熟悉Spring Cloud或者微服务的都知道，微服务中最忌讳的就是出现单点的情况。

所以针对同一个服务我们一般会部署2个或者更多个实例。在K8S中，则是会部署多个Pod副本，组成一个Pod集群来对外提供服务。

而我们前面提过，K8S会为每一个Pod提供一个唯一的IP地址，客户端就需要通过每个Pod的唯一IP+容器端口来访问到具体的Pod，这样一来，如果客户端把调用地址写死，服务器就没有办法做负载均衡，而且，Pod重启之后IP地址是会变的，难道每次重启都要通知客户端IP变更吗？

为了解决这个问题，就要引出Service的概念了。
2.2 Service

Service是K8S中最核心的资源对象之一，就是用于解决上面提到的问题。我个人认为与Swarm中的Service概念没有太大的区别。

一旦Service被创建，K8S会为其分配一个集群内唯一的IP，叫做ClusterIP，而且在Service的整个生命周期中，ClusterIP不会发生变更，这样一来，就可以用与Docker Swarm类似的操作，建立一个ClusterIP到服务名的DNS域名映射即可。

值得注意的是，ClusterIP是一个虚拟的IP地址，无法被Ping，仅仅只限于在K8S的集群内使用。

而Service对客户端，屏蔽了底层Pod的寻址的过程。并且由kube-proxy进程将对Service的请求转发到具体的Pod上，具体到哪一个，由具体的调度算法决定。这样以来，就实现了负载均衡。

而Service是怎么找到Pod的呢？这就需要继续引入另外一个核心概念Label了。
2.3 Label

Lable本质上是一个键值对，具体的值由用户决定。Lable就是标签，可以打在Pod上，也可以打到Service上。总结来说，Label与被标记的资源是一个一对多的关系。

例如，我们给上面所描述的Pod打上了role=serviceA的标签，那么只需要在Service中的Label Selector中加入刚刚那个标签，这样一来，Service就可以通过Label Selector找到打了同一Label的Pod副本集了。

接下来，再简单的介绍一下其他的K8S核心概念。
2.4 Replica Set

上面提到过部署多个Pod，是怎么一回事呢？K8S最开始有一个概念叫Replication Controller，不过现在已经慢慢的被Replica Set所替代，RS也叫下一代的RC。简单来说Replica Set定义了一种期望的场景，即让任何时候集群内的Pod副本数量都符合预期的值。

一旦被创建，集群就会定期的检测当前存活的Pod数量，如果多了，集群就会停掉一些Pod。相反，如果少了就会创建一些Pod。这样一来可以避免什么问题呢？假设某个服务有两个实例在运行，其中一个意外挂掉了，如果我们设置了副本数量是2，那么集群就会自动创建一个Pod，以保证集群内始终有两个Pod在运行。

K8S的东西就简单的介绍这么多，接下来让我们进入集群的搭建环节。
3. 搭建K8S的准备工作

不知道从哪篇博客开始，不是很愿意写这种纯TODO类的博文，但是我自己躺坑之后发现，我自己这个还真是我目前见过最简单的。

我看到的有些安装分了很多种情况，但是当一个初学者来看的时候，可能反而会让他看懵逼。所以接下来的安装会有些硬核。不分情况，就只有一种情况，一把梭安装就完事。

    系统 版本 Ubuntu 18.04
    K8S 版本 v1.16.3
    Docker 版本 v19.03.5
    Flannel 版本 v0.11.0

如果你问我，如果没有机器看了你的文章也能的拥有自己的集群吗？那么请看下图…
3.1 准备工作

我们先假设以下的情况成立。

    机器：有2-3台物理机或虚拟机
    系统：Ubuntu 18.04 且已换好国内的源

如果以上基本不成立，本篇文章到此结束，谢谢观看…
3.2 安装Docker

我也不需要介绍各种情况了，直接登上机器，创建一个shell脚本，例如叫install_docker.sh，一把梭代码如下。

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates 
curl gnupg-agent software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
sudo apt-key fingerprint 0EBFCD88
sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt-get update
sudo apt-get -y install docker-ce docker-ce-cli containerd.io

然后执行sh install_docker.sh，等待命令跑完，验证docker是否安装好即可。直接敲docker + 回车。
3.3 安装Kubernetes

同理，新建一个shell脚本，例如install_k8s.sh。一把梭代码如下。

sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-get update
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
sudo apt-get install -y kubelet kubeadm kubectl --allow-unauthenticated

然后执行sh install_k8s.sh，等待命令跑完，验证k8s是否安装好即可。直接敲kubectl + 回车。
3.4 关闭Swap

先给出一把梭，不要耽误了正在安装的老铁。为什么要关闭后面再说。

    暂时关闭 直接使用命令sudo swapoff -a，但是重启之后会生效。会导致k8s无法正常运行。永久关闭 建议一劳永逸，sudo vim /etc/fstab将有swap.img那行注释掉，保存即可。

那么，swap是啥呢？它是系统的交换分区，你可以理解为虚拟内存。当系统内存不足的时候，会将一部分硬盘空间虚拟成内存使用。那为什么K8S需要将其关掉呢？可以从下图看看访问内存和访问硬盘速度上的差异就知道了。

总的来说是为了性能考虑，所以就需要避免开启swap交换，K8S希望所有的服务都不应该超过集群或节点CPU和内存的限制。
4. 初始化Master节点

到这，准备工作就完成了，可以开始安装K8S的master节点了，登上要作为master节点的机器。
4.1 设置HostName

老规矩，先上命令，再说为什么要设置。

sudo hostnamectl set-hostname master-node

自定义修改了主机名，在之后查看集群内节点时，每个节点的名字就不会显示K8S自动生成的名字，便于查看和记忆。例如，在其他的Node节点你可以将master-node改为slave-node-1或worker-node-2，效果如下。
4.2 初始化集群

在机器上执行如下命令。

sudo kubeadm init --pod-network-cidr=10.244.0.0/16

然后，抱起吉他，等待命令执行完。
动图封面

这里需要特别注意一下。这个命令执行完成之后，会打印一个有kubeadm join的命令，需要保存下来。

大概长这样。

    kubeadm join 你的IP地址:6443 --token 你的TOKEN --discovery-token-ca-cert-hash sha256:你的CA证书哈希

顾名思义，这个命令用于其他节点加入到集群中，而且Token是有时效性的，过期时间一般是86400000毫秒。

如果失效，就需要重新生成。如果你真的又没有保存，又失效了…我还是给你准备了两个补救措施。如果命令保存下来了，那么请直接跳过这两个补救措施。

    token. 通过命令Kubeadm token list找回
    ca-cert. 执行命令openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'找回

4.3 普通用户可执行

把下面的指令一把梭即可。

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

主要是，为了不那么麻烦，在控制节点上执行kubectl这类的命令时，不用每次都sudo。
4.4 安装网络通信插件

执行如下命令，安装网络插件Flannel。

sudo kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

可以看到，如果不安装Flannel，我们刚刚Init好的Master节点会处于NOT_READY的状态。安装好之后，可以通过命令kubectl get nodes来查看所有的节点的状态。也可以通过kubectl get pods --all-namespaces来查看当前集群中所有Pod的状态。这里需要注意的是，只有在master节点是READY，所有Pod的状态是RUNNING之后，才可以进行下一步。

为什么要装网络插件呢？

那是因为K8S要求集群内的所有节点之间的Pod网络是互通的。换句话说，Flannel可以让集群内不同节点上的容器都有一个在当前集群内唯一的虚拟IP地址。这样以来，就可以实现，跨节点的Pod与Pod直接通信。

这样一来，将复杂的网络通信，简单的变成了两个IP地址之间的通信。这主要是通过虚拟二层网络实现的。看似是这个节点的Pod直接和另一个节点上的Pod进行了通信，最终还是通过节点的物理网卡流出的。
5. Slave节点加入集群

到此，一个单点的集群就已经搭建好了。现在我们要做的是，登录准备好的另一台（我只有两台，如果你有3台或者4天，把这个章节反复走几次就好了）服务器。
5.1 设置HostName

执行如下命令。

sudo hostnamectl set-hostname slave-node

因为当前节点不是master了，所以主机名设置成了slave-node。
5.2 加入集群

重点来了，执行上一章节生成的kubeadm join命令即可。等待执行完毕之后，就可以在master节点上通过命令kubectl get nodes看到slave-node已经加入了集群。

对于Slave节点的操作就没了。


第一篇资料

Kubeadm 手动搭建kubernetes 集群

目录

    K8S 组件构成
    环境准备 (以ubuntu系统为例）
        1. kubernetes集群机器
        2. 安装 docker、 kubeadm、kubelet、kubectl
            2.1 在每台机器上安装 docker
            2.2 每台机器上安装 kubelet 、kubeadm 、kubectl
    创建 kubernetes 集群
        kubeadm
        在 master 节点 init 集群
        在worker 节点执行命令 join 到集群
        安装 Pod Network (在 master 节点 flannel/Calico 网络插件)
    部署一个简单示例

kubeadm是Kubernetes官方提供的用于快速安装Kubernetes集群的工具，通过将集群的各个组件进行容器化安装管理，通过kubeadm的方式安装集群比二进制的方式安装要方便不少。

安装参考- https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

每一个节点主机上包括master节点都要手动安装并运行docker，同时也都要手动安装并运行kubelet。如果将第一个节点初始化为master节点，在执行初始化这个步骤，其实就是通过kubeadm工具将API Server、etcd、controller-manager、scheduler各组件运行为Pod，也就是跑在docker上。而其他node节点，因已经运行了kubelet、docker组件，剩下的kube-proxy组件也是要运行在Pod上。

kubeadm
K8S 组件构成#

    kubectl

    kubeadm

    K8s Master
        kubelet
        kube-proxy
        kube-apiserver
        kube-scheduler
        kube-controller-manager
        etcd

    K8s Node
        kubelet
        kube-proxy

    calico

    coredns

环境准备 (以ubuntu系统为例）#
1. kubernetes集群机器#
机器IP 	机器hostname 	K8s集群角色 	机器操作系统
172.20.249.16 	172-20-249-16 	master 	ubuntu16.04
172.20.249.17 	172-20-249-17 	node 	ubuntu16.04
172.20.249.18 	172-20-249-18 	node 	ubuntu16.04

    使用如下命令设置hostname: (非必须)

# 172.20.249.16

hostnamectl --static set-hostname k8s-master

# 172.20.249.17

hostnamectl --static set-hostname k8s-node-01

# 172.20.249.18

hostnamectl --static set-hostname k8s-node-02

Kubernetes v1.8+ 要求关闭系统 Swap，请在所有节点利用以下指令关闭 （否则kubelet会出错！）

swapoff -a && sed -i '/ swap / s/^/#/' /etc/fstab
2. 安装 docker、 kubeadm、kubelet、kubectl#
2.1 在每台机器上安装 docker#

# step 1: 安装必要的一些系统工具

sudo apt-get update

sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common


# step 2: 安装GPG证书

curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -


# Step 3: 写入软件源信息

sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"


# Step 4: 更新并安装 Docker-CE (可指定版本)

sudo apt-get -y update

sudo apt-get -y install docker-ce


sudo apt-get -y install docker-ce=17.03.0~ce-0~ubuntu-xenial

2.2 每台机器上安装 kubelet 、kubeadm 、kubectl#

    kubeadm: the command to bootstrap the cluster.
    kubectl: the command line util to talk to your cluster
    kubelet: the component that runs on all of the machines in your cluster and does things like starting pods and containers.

apt-get update && apt-get install -y apt-transport-https


# 安装 GPG 证书

curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add -


# 写入软件源；注意：我们用系统代号为 bionic，但目前阿里云不支持，所以沿用 16.04 的 xenial

cat <<EOF >/etc/apt/sources.list.d/kubernetes.list

deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main

EOF


apt-get update


apt-get install -y kubelet kubeadm kubectl


# 指定版本

apt-get install -y kubelet=1.18.8-00 kubeadm=1.18.8-00 kubectl=1.18.8-00

install-kube
创建 kubernetes 集群#
kubeadm#

kubeadm是一个构建k8s集群的工具。它提供的kubeadm init和 kubeadm join 两个命令是快速构建k8s集群的最佳实践。 其次，kubeadm工具只为构建最小可用集群，它只关心集群中最基础的组件，至于其他的插件（比如dashboard、CNI等）则不会涉及

    kubeadm init to bootstrap the initial Kubernetes control-plane node.

    kubeadm join to bootstrap a Kubernetes worker node or an additional control plane node, and join it to the cluster.

    kubeadm upgrade to upgrade a Kubernetes cluster to a newer version.

    kubeadm reset to revert any changes made to this host by kubeadm init or kubeadm join.

更多了解 kubeadm - https://www.cnblogs.com/shoufu/p/13047723.html
在 master 节点 init 集群#

kubeadm 初始化整个集群的过程，会生成相关的各种证书、kubeconfig 文件、bootstraptoken 等等

注意： 如果使用直接使用kubeadm init，会使用默认配置（如下）

kubeadm config print init-defaults --kubeconfig ClusterConfiguration > kubeadm.yml 可打印默认配置

apiVersion: kubeadm.k8s.io/v1beta2

bootstrapTokens:

- groups:

  - system:bootstrappers:kubeadm:default-node-token

  token: abcdef.0123456789abcdef

  ttl: 24h0m0s

  usages:

  - signing

  - authentication

kind: InitConfiguration

localAPIEndpoint:

  advertiseAddress: 1.2.3.4

  bindPort: 6443

nodeRegistration:

  criSocket: /var/run/dockershim.sock

  name: k8s-master

  taints:

  - effect: NoSchedule

    key: node-role.kubernetes.io/master

---

apiServer:

  timeoutForControlPlane: 4m0s

apiVersion: kubeadm.k8s.io/v1beta2

certificatesDir: /etc/kubernetes/pki

clusterName: kubernetes

controllerManager: {}

dns:

  type: CoreDNS

etcd:

  local:

    dataDir: /var/lib/etcd

imageRepository: k8s.gcr.io

# 默认情况下kubeadm会到k8s.gcr.io拉取镜像，不过对于一些私有化部署（比如国内存在墙的情况下，上面的地址是访问不到的），就需要自定义镜像地址了 如： imageRepository: registry.aliyuncs.com/google_containers

kind: ClusterConfiguration

kubernetesVersion: v1.18.0

networking:

  dnsDomain: cluster.local

  podSubnet: 10.244.0.0/16 # 添加该配置

  serviceSubnet: 10.96.0.0/12

scheduler: {}


修改配置文件后， 执行命令 kubeadm init --config kubeadm.yml即可

或者 直接传递参数执行 （如下）

kubeadm init --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16




#选择flannel作为 Pod 的网络插件，所以需要指定 --pod-network-cidr=10.244.0.0/16

#选择flannel作为 Pod 的网络插件，所以需要指定 --pod-network-cidr=192.168.0.0/16

参数说明：

--apiserver-advertise-address：这个参数指定了监听的API地址。若没有设置，则使用默认网络接口。


--apiserver-bind-port：这个参数指定了API服务器暴露出的端口号，默认是6443


--kubernetes-version：指定kubeadm安装的kubernetes版本。这个是很重要的，因为默认情况下kubeadm会安装与它版本相同的kubernetes版本


--image-repository 可以指定国内的镜像仓库。 默认k8s.gcr.io 国内无法访问


-- token-ttl：令牌被删除前的时间，默认是24h。kubeadm初始化完毕后会生成一个令牌，让其他节点能够加入集群，过时之后这个令牌会自动删除。如果设置为0之后令牌就永不过期


如下所示，kubeadm init 会 pull 必要的镜像，可能时间会比较长 (kubeadm config images pull 可测试是否可以拉取镜像，如果加了 --image-repository registry.aliyuncs.com/google_containers，不会担心在国内拉取镜像问题)

user@k8s-master:~$ kubeadm config images list

k8s.gcr.io/kube-apiserver:v1.18.9

k8s.gcr.io/kube-controller-manager:v1.18.9

k8s.gcr.io/kube-scheduler:v1.18.9

k8s.gcr.io/kube-proxy:v1.18.9

k8s.gcr.io/pause:3.2

k8s.gcr.io/etcd:3.4.3-0

k8s.gcr.io/coredns:1.6.7


init 完后，可以看到如下提示：

master-init

按照提示在 master 节点执行以下命令: (否则会出错)

mkdir -p $HOME/.kube

sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

此时， master 处于ready 状态

NAME         STATUS   ROLES    AGE   VERSION

k8s-master   Ready    master   14m   v1.18.6

在worker 节点执行命令 join 到集群#

拷贝在 master 节点 init 后的 join 命令，在其他两个 worker 节点执行:

kubeadm join 172.20.249.16:6443 --token cma8ob.ow9sfv5erqgkkp30 \

    --discovery-token-ca-cert-hash sha256:def379576eacaddbb4bbf4ca12fbb8a0b77383e4521cbf238f21c8dd3cb80fab

可以看到该节点已经加入到集群中去了，然后我们把 master 节点的~/.kube/config文件拷贝到当前节点对应的位置即可使用 kubectl 命令行工具了。

mkdir -p $HOME/.kube

# copy master "/etc/kubernetes/admin.conf"

sudo scp root@172.20.249.16:/etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config

要注意将上面的加入集群的命令保存下面，如果忘记，可以使用以下命令获取

kubeadm token create --print-join-command
安装 Pod Network (在 master 节点 flannel/Calico 网络插件)#

在 master 节点查看集群情况,可以看到节点的 status 还是 NotReady，这是由于还没有网络插件。

以 flannel插件 为例,在 master 节点 执行

#For Kubernetes v1.7+

wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

kubectl apply -f  kube-flannel.yml

Calico插件- 参考 https://docs.projectcalico.org/getting-started/kubernetes/quickstart

等待所有的 pod 都是 running 状态，可以看到所有 node 的 status 是 running 的状态，这时 kubernetes 集群就搭建好了。

node-ready

至此3个节点的集群搭建完成，后续可以继续添加node节点，或者部署dashboard、helm包管理工具、EFK日志系统、Prometheus Operator监控系统、rook+ceph存储系统等组件
部署一个简单示例#

kubectl create -f nginx-deployment.yaml

apiVersion: apps/v1

kind: Deployment

metadata:

  name: nginx-deployment

  labels:

    app: nginx

spec:

  selector:

    matchLabels:

      app: nginx

  replicas: 3

  strategy:

    type: RollingUpdate

  template:

    metadata:

      labels:

        app: nginx

    spec:

      containers:

        - name: nginx

          image: nginx:latest

          ports:

            - containerPort: 80

发布服务,暴露端口

kubectl expose deployment nginx-deployment --port=80 --type=LoadBalancer
